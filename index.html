<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>Animal Emotion AI</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.14.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

<style>
body {
  margin: 0;
  background: #0b0f1a;
  color: white;
  font-family: system-ui, sans-serif;
  overflow: hidden;
}

video, canvas {
  position: absolute;
  inset: 0;
  width: 100%;
  height: 100%;
  object-fit: cover;
}

/* video MUST be visible on mobile */
video { opacity: 0.02; }

.topbar {
  position: fixed;
  top: 12px;
  left: 50%;
  transform: translateX(-50%);
  z-index: 10;
}

button {
  background: rgba(255,255,255,0.15);
  color: white;
  border: 1px solid rgba(255,255,255,0.25);
  backdrop-filter: blur(10px);
  border-radius: 14px;
  padding: 12px 18px;
  font-size: 15px;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="topbar">
  <button id="startBtn">üì∑ Start Camera</button>
</div>

<video id="video" playsinline muted></video>
<canvas id="canvas"></canvas>

<script>
const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const ctx = canvas.getContext("2d");
const startBtn = document.getElementById("startBtn");

let model = null;
let running = false;
let lastBoxes = [];

startBtn.onclick = async () => {
  startBtn.disabled = true;
  startBtn.textContent = "Starting camera...";
  await startCamera();
};

async function startCamera() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: { ideal: "environment" } },
      audio: false
    });

    video.srcObject = stream;

    video.onloadedmetadata = async () => {
      await video.play();
      resizeCanvas();
      model = await cocoSsd.load();
      running = true;
      detectLoop();
    };
  } catch (err) {
    alert("Camera failed. Please allow camera permission and reload.");
    console.error(err);
  }
}

function resizeCanvas() {
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;
}

function movement(box, last) {
  if (!last) return 0;
  return Math.abs(box[0]-last[0]) + Math.abs(box[1]-last[1]);
}

function emotion(move, sizeChange) {
  if (move > 25 && sizeChange > 1800) return { text:"Angry üò†", conf:0.85 };
  if (move > 15) return { text:"Stressed üò®", conf:0.7 };
  if (move > 8) return { text:"Playful üêæ", conf:0.6 };
  if (move < 4) return { text:"Calm üôÇ", conf:0.75 };
  return { text:"Alert üëÄ", conf:0.5 };
}

async function detectLoop() {
  if (!running) return;

  const predictions = await model.detect(video);
  ctx.clearRect(0,0,canvas.width,canvas.height);

  predictions.forEach((p,i)=>{
    if (!["dog","cat","bird","horse"].includes(p.class)) return;

    const [x,y,w,h] = p.bbox;
    const last = lastBoxes[i];
    const move = movement(p.bbox,last);
    const sizeChange = last ? Math.abs(w*h - last[2]*last[3]) : 0;

    const e = emotion(move,sizeChange);

    ctx.strokeStyle = "#00ffd5";
    ctx.lineWidth = 2;
    ctx.strokeRect(x,y,w,h);

    ctx.fillStyle = "rgba(0,0,0,0.65)";
    ctx.fillRect(x,y-44,w,44);

    ctx.fillStyle = "white";
    ctx.font = "bold 14px sans-serif";
    ctx.fillText(p.class.toUpperCase(),x+6,y-26);

    ctx.fillStyle = "#00ffd5";
    ctx.font = "13px sans-serif";
    ctx.fillText(`${e.text} (${Math.round(e.conf*100)}%)`,x+6,y-10);
  });

  lastBoxes = predictions.map(p=>p.bbox);
  requestAnimationFrame(detectLoop);
}
</script>

</body>
</html>
